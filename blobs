#!/usr/bin/env python
from typing import (
    TypeVar,
    Literal,
    Union,
    Set,
    Tuple,
    List,
    Dict,
    Optional,
    Mapping,
    MutableMapping,
    Iterable,
    Sequence,
    Callable,
    Any,
    BinaryIO,
)
import sys
import os
import pyaconf
import argparse
import hashlib
import tempfile
from pathlib import Path
import itertools
import json
import subprocess
import random
import bisect
import string


def logg(s: str) -> None:
    print(s, file=sys.stderr)


def crange(start, stop, step, modulo):
    for i in range(start, stop, step):
        yield i % modulo


class ConsistentHash:
    def __init__(
        self,
        nodes: Mapping[str, Dict[str, Any]],
        hash_size: int = 1021,
        num_replicas: int = 3,
    ) -> None:
        self.nodes = {k: v for k, v in nodes.items()}
        self.hash_size = hash_size
        self.num_replicas = num_replicas
        assert 1 <= self.num_replicas <= len(self.nodes.items())
        self.ring = sorted(
            sum(
                (
                    [
                        (self.hash_fun(k + str(i)), [k])
                        for i in range(node["num_vnodes"])
                    ]
                    for k, node in nodes.items()
                ),
                [],
            )
        )
        for i in range(len(self.ring)):
            rs = self.ring[i][1]
            for j in crange(i - 1, i - len(self.ring), -1, len(self.ring)):
                if len(rs) == num_replicas:
                    break
                if self.ring[j][1][0] not in rs:
                    rs.append(self.ring[j][1][0])
        self.ring_hash_codes = [a for a, b in self.ring]

    def hash_fun(self, val: str) -> int:
        return (
            int.from_bytes(hashlib.sha1(val.encode("utf-8")).digest()[:4], "little")
            % self.hash_size
        )

    def loc(self, sha: str) -> List[str]:
        return self.ring[
            (bisect.bisect_right(self.ring_hash_codes, self.hash_fun(sha)) - 1)
            % len(self.ring_hash_codes)
        ][1]


class Blobs:
    def __init__(self, config: Mapping[str, Any]) -> None:
        self.commands = dict(
            init=self.init,
            sha=self.sha,
            loc=self.loc,
            put=self.put,
            get=self.get,
            push=self.push,
            pull=self.pull,
            rehash=self.rehash,
            chk=self.chk,
        )
        self.nodes = config["nodes"]
        assert len(self.nodes) > 0
        self.local_node_id = config["node_id"]
        self.local_node = self.nodes[self.local_node_id]
        self.local_repo = self.local_node["repo"]
        self.num_replicas = config["num_replicas"]
        assert 1 <= self.num_replicas <= len(self.nodes)
        self.chash_size = config["chash_size"]
        self.subdir_depth = config["subdir_depth"]
        self.pow_subdirs = config["pow_subdirs"]
        self.rehash_verifies_sha = config["rehash_verifies_sha"]
        self.chash = ConsistentHash(
            self.nodes, num_replicas=self.num_replicas, hash_size=self.chash_size
        )

    def _num_subdirs(self):
        return 16 ** self.pow_subdirs

    def _jsondump(self, val: Dict[str, Any]):
        json.dump(val, sys.stdout)
        print()

    def _is_sha(self, sha: str) -> bool:
        return len(sha) == 64 and all(c in string.hexdigits for c in sha)

    def _file_copy(
        self,
        src: Union[Path, str, int],
        dst: Union[Path, str, int],
        block_size: int = 65536,
    ) -> str:
        # TODO: if put on the same FS, use hardlink instead of copy
        hasher = hashlib.sha256()
        with open(src, "rb") as fsrc:
            with open(dst, "wb") as fdst:
                buf = fsrc.read(block_size)
                while len(buf) != 0:
                    hasher.update(buf)
                    fdst.write(buf)
                    buf = fsrc.read(block_size)
        return hasher.hexdigest()

    def _file_sha(self, src: Union[Path, str, int], block_size: int = 65536):
        hasher = hashlib.sha256()
        with open(src, "rb") as fsrc:
            buf = fsrc.read(block_size)
            while len(buf) != 0:
                hasher.update(buf)
                buf = fsrc.read(block_size)
        return hasher.hexdigest()

        return self._sha_and_copy(src)

    def _sha_path(self, sha: str) -> Path:
        return Path(
            *[
                sha[i * self.pow_subdirs : (i + 1) * self.pow_subdirs]
                for i in range(self.subdir_depth)
            ],
            sha,
        )

    def _tmp_dir(self, repo: Path) -> Path:
        return Path(repo, "tmp")

    def _cache_dir(self, repo: Path) -> Path:
        return Path(repo, "cache")

    def _store_dir(self, repo: Path, node_id: str = None) -> Path:
        p = Path(repo, "store")
        if node_id is not None:
            p = Path(p, node_id)
        return p

    def _rsync(
        self, src: str, dst: str, remove_src: bool = False, compress: bool = False
    ) -> None:
        cmd = ["rsync", "-acIh"]  # -racvhzI
        if compress:
            cmd.append("-z")
        if remove_src:
            cmd.append("--remove-source-files")
        cmd.append(src)
        cmd.append(dst)
        logg(f"rsync: {cmd=}")
        r = subprocess.run(cmd, check=True)

    def _synchronize(self, cmd: str) -> None:
        for node_id, node in self.nodes.items():
            if self.local_node_id != node_id:
                remote_spec = "" if node["local"] else f"{node['user']}@{node['host']}:"
                if cmd in ("pull"):
                    self._rsync(
                        remote_spec
                        + str(self._store_dir(node["repo"], self.local_node_id)),
                        str(self._store_dir(self.local_repo)),
                        True,
                        node["compress"],
                    )
                elif cmd in ("push"):
                    self._rsync(
                        str(self._store_dir(self.local_repo, node_id)),
                        remote_spec + str(self._store_dir(node["repo"])),
                        True,
                        node["compress"],
                    )

    def pull(self, args: Iterable[str]) -> None:
        self._synchronize("pull")
        self._jsondump(
            dict(
                op="pull", local_node_id=self.local_node_id, local_repo=self.local_repo
            )
        )

    def push(self, args: Iterable[str]) -> None:
        self._synchronize("push")
        self._jsondump(
            dict(
                op="push", local_node_id=self.local_node_id, local_repo=self.local_repo
            )
        )

    def _init_dir(self, path: Path, depth: int) -> None:
        fmt = "%%0.%dx" % self.pow_subdirs
        for i in range(self._num_subdirs()):
            p = Path(path, fmt % i)
            os.makedirs(p, exist_ok=True)
            if depth > 1:
                self._init_dir(p, depth - 1)

    def init(self, args: Iterable[str]) -> None:
        os.makedirs(self.local_repo, exist_ok=True)
        os.makedirs(self._tmp_dir(self.local_repo), exist_ok=True)
        os.makedirs(self._cache_dir(self.local_repo), exist_ok=True)
        self._init_dir(self._cache_dir(self.local_repo), self.subdir_depth)
        for node_id in self.nodes.keys():
            os.makedirs(self._store_dir(self.local_repo, node_id), exist_ok=True)
            self._init_dir(self._store_dir(self.local_repo, node_id), self.subdir_depth)
        self._jsondump(
            dict(
                op="init", local_node_id=self.local_node_id, local_repo=self.local_repo
            )
        )

    def sha(self, args: Iterable[str]) -> None:
        for file_path in args:
            sha = self._file_sha(file_path)
            locs = self.chash.loc(sha)
            self._jsondump(
                dict(
                    op="sha",
                    local_node_id=self.local_node_id,
                    local_repo=self.local_repo,
                    file_path=file_path,
                    sha=sha,
                    locs=locs,
                )
            )

    def loc(self, args: Iterable[str]) -> None:
        for sha in args:
            if not self._is_sha(sha):
                logg(f"Invalid sha {sha=}")
                continue
            locs = self.chash.loc(sha)
            self._jsondump(
                dict(
                    op="loc",
                    local_node_id=self.local_node_id,
                    local_repo=self.local_repo,
                    sha=sha,
                    locs=locs,
                )
            )

    def _place(self, path: Path, sha: str) -> None:
        locs = self.chash.loc(sha)
        if self.local_node_id not in locs:
            p = Path(self._cache_dir(self.local_repo), self._sha_path(sha))
            if not p.exists():
                os.link(path, p)
        for node_id in locs:
            p = Path(self._store_dir(self.local_repo, node_id), self._sha_path(sha))
            if not p.exists():
                os.link(path, p)

    def put(self, args: Iterable[str]) -> None:
        for file_path in args:
            temp_fd, temp_path = tempfile.mkstemp(
                dir=self._tmp_dir(self.local_repo), text=False
            )
            sha = self._file_copy(file_path, temp_fd)
            self._place(Path(temp_path), sha)
            os.unlink(temp_path)
            self._jsondump(
                dict(
                    op="put",
                    local_node_id=self.local_node_id,
                    local_repo=self.local_repo,
                    file_path=file_path,
                    sha=sha,
                    locs=self.chash.loc(sha),
                )
            )

    def get(self, args: Iterable[str]) -> None:
        for sha in args:
            if not self._is_sha(sha):
                logg(f"Invalid sha {sha=}")
                continue
            locs = self.chash.loc(sha)
            sp = self._sha_path(sha)
            cpath = Path(self._cache_dir(self.local_repo), sp)
            spath = Path(self._store_dir(self.local_repo, self.local_node_id), sp)
            if cpath.exists():
                dpath = cpath
            elif spath.exists():
                dpath = spath
            else:
                dpath = spath if self.local_node_id in locs else cpath
                # TODO: if the host isn't available, get another replica
                src_node_id = locs[random.randrange(len(locs))]
                src_node = self.nodes[src_node_id]
                remote_spec = (
                    ""
                    if src_node["local"]
                    else f"{src_node['user']}@{src_node['host']}:"
                )
                spath = Path(self._store_dir(src_node["repo"], src_node_id), sp)
                self._rsync(
                    remote_spec + str(spath), str(dpath), False, src_node["compress"]
                )
            self._jsondump(
                dict(
                    op="get",
                    local_node_id=self.local_node_id,
                    local_repo=self.local_repo,
                    file_path=str(dpath),
                    sha=sha,
                    locs=locs,
                )
            )

    def _rehash(
        self, path: Path, temp_path: Path, node_id: str, mode: str, verify_sha: bool
    ) -> None:
        for d in os.scandir(path):
            if d.is_dir():
                # logg(f"D1 {d.path=}, {d.name=}")
                self._rehash(Path(d.path), temp_path, node_id, mode, verify_sha)
            else:
                sha = Path(d.path).stem
                if not self._is_sha(sha):
                    logg(f"Invalid sha {sha=}, {d.path=}, {d.name=}")
                    continue
                locs = self.chash.loc(sha)
                # logg(f"F1 {d.path=}, {d.name=}, {sha=}, {locs=}")
                is_misplaced = (
                    node_id in locs if mode == "cache" else node_id not in locs
                )
                if is_misplaced:
                    logg(f"Misplaced {sha=}, {d.path=}, {d.name=}, {locs=}")
                    correct_sha = sha
                    if verify_sha:
                        correct_sha = self._file_sha(d.path)
                    if sha != correct_sha:
                        logg(f"Bad sha {d.path=}, {d.name=}, {sha=}, {correct_sha=}")
                        continue
                    p = Path(temp_path, d.name)
                    if p.exists():
                        os.unlink(p)
                    os.rename(d.path, p)

    def rehash(self, args: Iterable[str]) -> None:
        temp_path = Path(tempfile.mkdtemp(dir=self._tmp_dir(self.local_repo)))
        self._rehash(
            self._cache_dir(self.local_repo),
            temp_path,
            self.local_node_id,
            "cache",
            self.rehash_verifies_sha,
        )
        for node_id in self.nodes.keys():
            self._rehash(
                self._store_dir(self.local_repo, node_id),
                temp_path,
                node_id,
                "store",
                self.rehash_verifies_sha,
            )
        for d in os.scandir(temp_path):
            logg(f"F2 {d.path=}, {d.name=}")
            sha = Path(d.path).stem
            self._place(Path(d.path), sha)
            os.unlink(d.path)
            self._jsondump(
                dict(
                    op="rehash",
                    local_node_id=self.local_node_id,
                    local_repo=self.local_repo,
                    sha=sha,
                )
            )
        os.rmdir(temp_path)

    def _chk(self, path: Path, node_id: str, mode: str) -> None:
        for d in os.scandir(path):
            if d.is_dir():
                # logg(f"D1 {d.path=}, {d.name=}")
                self._chk(Path(d.path), node_id, mode)
            else:
                sha = Path(d.path).stem
                if not self._is_sha(sha):
                    logg(f"Invalid sha {sha=}, {d.path=}, {d.name=}")
                    continue
                correct_sha = self._file_sha(d.path)
                if sha != correct_sha:
                    logg(f"Bad sha {d.path=}, {d.name=}, {sha=}, {correct_sha=}")
                    continue
                locs = self.chash.loc(sha)
                # logg(f"F1 {d.path=}, {d.name=}, {sha=}, {locs=}")
                if node_id in locs if mode == "cache" else node_id not in locs:
                    logg(f"Misplaced {sha=}, {d.path=}, {d.name=}, {locs=}")

    def chk(self, args: Iterable[str]) -> None:
        self._chk(self._cache_dir(self.local_repo), self.local_node_id, "cache")
        for node_id in self.nodes.keys():
            self._chk(self._store_dir(self.local_repo, node_id), node_id, "store")

    def command(self, cmd: str, args: Iterable[str]) -> None:
        self.commands[cmd](args)


def main() -> None:
    cmd = sys.argv[1]
    args = sys.argv[2:]
    config = pyaconf.load(os.environ["BLOBS_CONFIG"])
    blobs = Blobs(config)
    blobs.command(cmd, args)


main()
